{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0aa52fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-07 22:30:29.240324: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-07 22:30:29.266083: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-07 22:30:29.266110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-07 22:30:29.266838: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-07 22:30:29.271297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-07 22:30:29.962384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification \n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ordered_set import OrderedSet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5efe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURACI√ìN DE TENSORFLOW 2.15.1 CON CUDA 12.2\n",
      "======================================================================\n",
      "\n",
      "GPUs disponibles: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-07 22:30:34.535837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-02-07 22:30:34.537687: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACI√ìN DE TENSORFLOW 2.15.1 CON CUDA 12.2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nGPUs disponibles: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  - {gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1dd306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øSoporte CUDA?:  True\n",
      "No se detect√≥ ninguna GPU. Revisa los drivers o la versi√≥n de CUDA.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Comprobar si TensorFlow fue compilado con soporte CUDA\n",
    "print(\"¬øSoporte CUDA?: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "# 2. Listar dispositivos f√≠sicos (GPUs) detectados\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detectadas: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\" - Nombre: {gpu}\")\n",
    "else:\n",
    "    print(\"No se detect√≥ ninguna GPU. Revisa los drivers o la versi√≥n de CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcee9b",
   "metadata": {},
   "source": [
    "### **Carga Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b0dc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43410, 3) (5426, 3) (5427, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./Kaggle/Go_Emotions/data/train.tsv', sep='\\t', header=None, names=['text','labels', 'code'])\n",
    "validation_df = pd.read_csv('./Kaggle/Go_Emotions/data/dev.tsv', sep='\\t', header=None, names=['text','labels', 'code'])\n",
    "test_df = pd.read_csv('./Kaggle/Go_Emotions/data/test.tsv', sep='\\t', header=None, names=['text','labels', 'code'])\n",
    "\n",
    "print(train_df.shape, validation_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ff81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(inplace=True, axis=1, labels=['code'])\n",
    "validation_df.drop(inplace=True, axis=1, labels=['code'])\n",
    "test_df.drop(inplace=True, axis=1, labels=['code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e89c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'admiration', 1: 'amusement', 2: 'anger', 3: 'annoyance', 4: 'approval', 5: 'caring', 6: 'confusion', 7: 'curiosity', 8: 'desire', 9: 'disappointment', 10: 'disapproval', 11: 'disgust', 12: 'embarrassment', 13: 'excitement', 14: 'fear', 15: 'gratitude', 16: 'grief', 17: 'joy', 18: 'love', 19: 'nervousness', 20: 'optimism', 21: 'pride', 22: 'realization', 23: 'relief', 24: 'remorse', 25: 'sadness', 26: 'surprise', 27: 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "decode_labels = {}\n",
    "i = 0\n",
    "with open('./Kaggle/Go_Emotions/data/emotions.txt', 'r') as decoding:\n",
    "    for line in decoding:\n",
    "        decode_labels[i] = line.strip()\n",
    "        i += 1\n",
    "print(decode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8d2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(label):\n",
    "    label = list(map(int, label.split(',')))\n",
    "    decoded = []\n",
    "    for x in label:\n",
    "        decoded.append(decode_labels.get(x))\n",
    "    return decoded\n",
    "\n",
    "train_df['labels'] = train_df['labels'].apply(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24566d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['labels'] = validation_df['labels'].apply(decoding)\n",
    "test_df['labels'] = test_df['labels'].apply(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9486aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "goemotions_to_fer = {\n",
    "    \"anger\": \"angry\",\n",
    "    \"annoyance\": \"angry\",\n",
    "    \"disapproval\": \"angry\", \n",
    "\n",
    "    \"disgust\": \"disgust\",\n",
    "\n",
    "    \"fear\": \"fear\",\n",
    "    \"nervousness\": \"fear\",\n",
    "\n",
    "    \"admiration\": \"happy\",\n",
    "    \"amusement\": \"happy\",\n",
    "    \"approval\": \"happy\",\n",
    "    \"caring\": \"happy\",\n",
    "    \"excitement\": \"happy\",\n",
    "    \"gratitude\": \"happy\",\n",
    "    \"joy\": \"happy\",\n",
    "    \"love\": \"happy\",\n",
    "    \"optimism\": \"happy\",\n",
    "    \"pride\": \"happy\",\n",
    "    \"relief\": \"happy\",\n",
    "\n",
    "    \"disappointment\": \"sad\",\n",
    "    \"embarrassment\": \"sad\",\n",
    "    \"grief\": \"sad\",\n",
    "    \"remorse\": \"sad\",\n",
    "    \"sadness\": \"sad\",\n",
    "\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"realization\": \"surprise\",\n",
    "    \"curiosity\": \"surprise\",  \n",
    "    \"confusion\": \"surprise\",\n",
    "\n",
    "    \"neutral\": \"neutral\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e737979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_fer(go_labels, mapping):\n",
    "    mapped = OrderedSet()\n",
    "    for label in go_labels:\n",
    "        if label in mapping:\n",
    "            mapped.add(mapping[label])\n",
    "    return list(mapped) if mapped else [\"neutral\"]  \n",
    "train_df['fer_labels'] = train_df['labels'].apply(lambda labs: map_to_fer(labs, goemotions_to_fer))\n",
    "validation_df['fer_labels'] = validation_df['labels'].apply(lambda labs: map_to_fer(labs, goemotions_to_fer))\n",
    "test_df['fer_labels'] = test_df['labels'].apply(lambda labs: map_to_fer(labs, goemotions_to_fer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "283c6c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fer_labels\n",
      "happy       16935\n",
      "neutral     14608\n",
      "angry        5579\n",
      "surprise     5367\n",
      "sad          3263\n",
      "disgust       793\n",
      "fear          726\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_labels = train_df['fer_labels'].explode().value_counts()\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "353ade22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['fer_label'] = train_df['fer_labels'].apply(lambda x: x[0])\n",
    "validation_df['fer_label'] = validation_df['fer_labels'].apply(lambda x: x[0])\n",
    "test_df['fer_label'] = test_df['fer_labels'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba48efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentiment_final'] = train_df['labels'].apply(lambda x: x[0])\n",
    "validation_df['sentiment_final'] = validation_df['labels'].apply(lambda x: x[0])\n",
    "test_df['sentiment_final'] = test_df['labels'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48bc7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {\n",
    "    \"angry\":0,\n",
    "    \"disgust\":1,\n",
    "    \"fear\":2,\n",
    "    \"happy\":3,\n",
    "    \"sad\":4,\n",
    "    \"surprise\":5,\n",
    "    \"neutral\":6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51a66e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "train_df['label_id'] = [map_dict[i] for i in train_df['fer_label']]\n",
    "validation_df['label_id'] = [map_dict[i] for i in validation_df['fer_label']]\n",
    "test_df['label_id'] = [map_dict[i] for i in test_df['fer_label']]\n",
    "\n",
    "num_labels = len(map_dict)\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf7a5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['label_id'].tolist()\n",
    "val_texts = validation_df['text'].tolist()\n",
    "val_labels = validation_df['label_id'].tolist()\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = test_df['label_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d47278d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))\n",
    "\n",
    "train_ds = train_ds.shuffle(1000).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(8).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0df71312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df, label_col):\n",
    "  min_count = df[label_col].value_counts().min()\n",
    "  min_count\n",
    "  df = (\n",
    "      df.groupby(label_col, group_keys=False)\n",
    "        .apply(lambda x: x.sample(min_count, random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "  )\n",
    "  df.head()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eedad2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, validation_df,test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0420337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = balance_classes(df, 'fer_label')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34deaeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_final\n",
       "happy       20599\n",
       "neutral     16545\n",
       "angry        6729\n",
       "surprise     5593\n",
       "sad          3264\n",
       "fear          792\n",
       "disgust       741\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['labels', 'fer_labels', 'label_id','sentiment_final'])\n",
    "df = df.rename(columns={\"fer_label\": \"sentiment_final\"})    \n",
    "\n",
    "df['sentiment_final'].value_counts()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "640de9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment_final\n",
       "0  My favourite food is anything I didn't have to...         neutral\n",
       "1  Now if he does off himself, everyone will thin...         neutral\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING           angry\n",
       "3                        To make her feel threatened            fear\n",
       "4                             Dirty Southern Wankers           angry"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73eb7d2",
   "metadata": {},
   "source": [
    "### **Entrenamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e92a54",
   "metadata": {},
   "source": [
    "#### **DistilBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c2da4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12708209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de emociones/clases: 7\n",
      "Clases: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['sentiment_final'])\n",
    "\n",
    "# N√∫mero de clases\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(f\"N√∫mero de emociones/clases: {num_labels}\")\n",
    "print(f\"Clases: {label_encoder.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e2f5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['label_encoded'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label_encoded']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c85bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para clasificaci√≥n de emociones con DistilBERT\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenizar\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d352f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Cargando DistilBERT tokenizer...\n",
      "‚úÖ Datasets creados - Train: 43410, Val: 10853\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì¶ Cargando DistilBERT tokenizer...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_dataset = EmotionDataset(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = EmotionDataset(\n",
    "    texts=val_texts,\n",
    "    labels=val_labels,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets creados - Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08980760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoaders creados - Batches por √©poca: 2714\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders creados - Batches por √©poca: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08348ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Cargando modelo DistilBERT preentrenado...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a98e3bc639454abb57c18ff8be3b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado en cuda\n",
      "üìä Par√°metros totales: 66,958,855\n",
      "üìä Par√°metros entrenables: 66,958,855\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì¶ Cargando modelo DistilBERT preentrenado...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Modelo cargado en {device}\")\n",
    "\n",
    "# Mostrar par√°metros del modelo\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Par√°metros totales: {total_params:,}\")\n",
    "print(f\"üìä Par√°metros entrenables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00f457",
   "metadata": {},
   "source": [
    "#### **Roberta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "541a0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72a1985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de emociones/clases: 7\n",
      "Clases: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['sentiment_final'])\n",
    "\n",
    "# N√∫mero de clases\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(f\"N√∫mero de emociones/clases: {num_labels}\")\n",
    "print(f\"Clases: {label_encoder.classes_}\")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['label_encoded'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label_encoded']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11883f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para clasificaci√≥n de emociones con RoBERTa\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenizar\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a7c3e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Cargando RoBERTa tokenizer...\n",
      "‚úÖ Datasets creados - Train: 43410, Val: 10853\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì¶ Cargando RoBERTa tokenizer...\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_dataset = EmotionDataset(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = EmotionDataset(\n",
    "    texts=val_texts,\n",
    "    labels=val_labels,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets creados - Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8abb0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoaders creados - Batches por √©poca: 2714\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders creados - Batches por √©poca: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c612cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Cargando modelo RoBERTa preentrenado...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbfd3b6385a4cfeab3af86ebcfa0efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado en cuda\n",
      "üìä Par√°metros totales: 124,651,015\n",
      "üìä Par√°metros entrenables: 124,651,015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüì¶ Cargando modelo RoBERTa preentrenado...\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Modelo cargado en {device}\")\n",
    "\n",
    "# Mostrar par√°metros del modelo\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Par√°metros totales: {total_params:,}\")\n",
    "print(f\"üìä Par√°metros entrenables: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb63baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.AdamW.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<generator object Module.parameters at 0x7f544c1ebb50>) with an unsupported type (<class 'generator'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Learning rate scheduler con warmup\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_linear_schedule_with_warmup\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/keras/src/optimizers/adamw.py:119\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[0;34m(self, learning_rate, weight_decay, beta_1, beta_2, epsilon, amsgrad, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     92\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    107\u001b[0m ):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    109\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    110\u001b[0m         clipnorm\u001b[38;5;241m=\u001b[39mclipnorm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_decay \u001b[38;5;241m=\u001b[39m weight_decay\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1282\u001b[0m, in \u001b[0;36mOptimizer._build_learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_build_learning_rate\u001b[39m(\u001b[38;5;28mself\u001b[39m, learning_rate):\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh:\n\u001b[0;32m-> 1282\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;66;03m# For DTensor\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     variable_creation \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdtensor\u001b[38;5;241m.\u001b[39mDVariable\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:395\u001b[0m, in \u001b[0;36m_BaseOptimizer._build_learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_learning_rate \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\n\u001b[1;32m    388\u001b[0m         current_learning_rate,\n\u001b[1;32m    389\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    390\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mcurrent_learning_rate\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    391\u001b[0m         trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m learning_rate\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloatx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<generator object Module.parameters at 0x7f544c1ebb50>) with an unsupported type (<class 'generator'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler con warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = total_steps // 10  # 10% de warmup\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41367e",
   "metadata": {},
   "source": [
    "#### **Distilroberta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e657d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22addb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd810ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de emociones/clases: 7\n",
      "Clases: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['sentiment_final'])\n",
    "\n",
    "# N√∫mero de clases\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(f\"N√∫mero de emociones/clases: {num_labels}\")\n",
    "print(f\"Clases: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fb351e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è NOTA: Emotion-DistilRoBERTa est√° pre-entrenado para estas emociones:\n",
      "   anger, disgust, fear, joy, neutral, sadness, surprise\n",
      "   Si tus clases son diferentes, el modelo se adaptar√° durante el fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚ö†Ô∏è NOTA: Emotion-DistilRoBERTa est√° pre-entrenado para estas emociones:\")\n",
    "print(\"   anger, disgust, fear, joy, neutral, sadness, surprise\")\n",
    "print(\"   Si tus clases son diferentes, el modelo se adaptar√° durante el fine-tuning.\")\n",
    "\n",
    "# Split train/validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['label_encoded'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label_encoded']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56835a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para clasificaci√≥n de emociones con Emotion-DistilRoBERTa\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenizar\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc247af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Cargando Emotion-DistilRoBERTa tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56c6efba9694537ae32677ae13aeea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfea90130fbe48cf8ed5828b42f4cf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a087abcd47074488b4c69649011eedcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbe01fef8264269b7c51e60a0dc9b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd1ca11cf054cb692849849ee9fa4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97dc901240284eec98e28f2f1d33731d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets creados - Train: 43410, Val: 10853\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì¶ Cargando Emotion-DistilRoBERTa tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('j-hartmann/emotion-english-distilroberta-base')\n",
    "\n",
    "train_dataset = EmotionDataset(\n",
    "    texts=train_texts,\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = EmotionDataset(\n",
    "    texts=val_texts,\n",
    "    labels=val_labels,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets creados - Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf20442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataLoaders creados - Batches por √©poca: 2714\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders creados - Batches por √©poca: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e866d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Cargando modelo Emotion-DistilRoBERTa preentrenado...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ff02278e0d4c60b32e7fae2fe0cd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a27ef6534384203b7b61ab80d9e5d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: j-hartmann/emotion-english-distilroberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado en cuda\n",
      "üìä Par√°metros totales: 82,123,783\n",
      "üìä Par√°metros entrenables: 82,123,783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0159592d1ff340c0a4bbe2474c70f04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nüì¶ Cargando modelo Emotion-DistilRoBERTa preentrenado...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'j-hartmann/emotion-english-distilroberta-base',\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True  # Permite cambiar el n√∫mero de clases\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Modelo cargado en {device}\")\n",
    "\n",
    "# Mostrar par√°metros del modelo\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üìä Par√°metros totales: {total_params:,}\")\n",
    "print(f\"üìä Par√°metros entrenables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e05ac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.AdamW.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<generator object Module.parameters at 0x7f546c4f4c10>) with an unsupported type (<class 'generator'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Learning rate scheduler con warmup\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_linear_schedule_with_warmup\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/keras/src/optimizers/adamw.py:119\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[0;34m(self, learning_rate, weight_decay, beta_1, beta_2, epsilon, amsgrad, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     92\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    107\u001b[0m ):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    109\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    110\u001b[0m         clipnorm\u001b[38;5;241m=\u001b[39mclipnorm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_decay \u001b[38;5;241m=\u001b[39m weight_decay\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1282\u001b[0m, in \u001b[0;36mOptimizer._build_learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_build_learning_rate\u001b[39m(\u001b[38;5;28mself\u001b[39m, learning_rate):\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh:\n\u001b[0;32m-> 1282\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_learning_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;66;03m# For DTensor\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     variable_creation \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdtensor\u001b[38;5;241m.\u001b[39mDVariable\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:395\u001b[0m, in \u001b[0;36m_BaseOptimizer._build_learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_learning_rate \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\n\u001b[1;32m    388\u001b[0m         current_learning_rate,\n\u001b[1;32m    389\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    390\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mcurrent_learning_rate\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    391\u001b[0m         trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m learning_rate\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloatx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/audio/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<generator object Module.parameters at 0x7f546c4f4c10>) with an unsupported type (<class 'generator'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler con warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = total_steps // 10  # 10% de warmup\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Configuraci√≥n de entrenamiento:\")\n",
    "print(f\"   Learning rate: {5e-6}\")\n",
    "print(f\"   √âpocas: {num_epochs}\")\n",
    "print(f\"   Total steps: {total_steps}\")\n",
    "print(f\"   Warmup steps: {warmup_steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
