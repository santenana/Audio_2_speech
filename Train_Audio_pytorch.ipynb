{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110f5528",
   "metadata": {},
   "source": [
    "### **Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44b3c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 10:06:21.847527: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-11 10:06:21.949738: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-11 10:06:23.391513: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu128\n",
      "2.7.1+cu128\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "from transformers import (\n",
    "     WhisperFeatureExtractor,\n",
    "     WhisperTokenizer,\n",
    "     WhisperProcessor,\n",
    "     WhisperForConditionalGeneration,\n",
    "     Seq2SeqTrainingArguments,\n",
    "     Seq2SeqTrainer)\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c451a",
   "metadata": {},
   "source": [
    "### **Parametros para Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "283e9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/whisper-small\" \n",
    "AUDIO_DIR = \"./Audios_ES\"\n",
    "METADATA_FILE = \"./final_metadata.csv\"\n",
    "OUTPUT_DIR = \"./whisper-finetuned-transcipt-es\"\n",
    "LANGUAGE = \"spanish\"\n",
    "TASK = \"transcribe\"\n",
    "SUPPORTED_FORMATS = ['.mp3', '.wav', '.flac', '.ogg', '.m4a', '.opus', '.wma']\n",
    "TARGET_SAMPLE_RATE = 16000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce1fda",
   "metadata": {},
   "source": [
    "### **Funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_multiformat(audio_path, target_sr=TARGET_SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Carga audio de cualquier formato y lo convierte a 16kHz mono\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Ruta al archivo de audio\n",
    "        target_sr: Sample rate objetivo (16000 para Whisper)\n",
    "    \n",
    "    Returns:\n",
    "        waveform: numpy array con el audio\n",
    "        sample_rate: sample rate del audio cargado\n",
    "    \"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    file_ext = Path(audio_path).suffix.lower()\n",
    "    \n",
    "    try:\n",
    "        if file_ext in ['.wav', '.flac']:\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            waveform = waveform.squeeze().numpy()\n",
    "        else:\n",
    "            waveform, sample_rate = librosa.load(\n",
    "                audio_path, \n",
    "                sr=target_sr, \n",
    "                mono=True\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando {audio_path}: {e}\")\n",
    "        try:\n",
    "            waveform, sample_rate = librosa.load(audio_path, sr=None, mono=True)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error cr√≠tico cargando {audio_path}: {e2}\")\n",
    "            return None, None\n",
    "    \n",
    "    if sample_rate != target_sr:\n",
    "        waveform = librosa.resample(\n",
    "            waveform, \n",
    "            orig_sr=sample_rate, \n",
    "            target_sr=target_sr\n",
    "        )\n",
    "        sample_rate = target_sr\n",
    "    \n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "\n",
    "def validate_audio_files(audio_dir, metadata_df):\n",
    "    \"\"\"\n",
    "    Valida que los archivos de audio existan y sean accesibles\n",
    "    \"\"\"\n",
    "    valid_files = []\n",
    "    invalid_files = []\n",
    "    \n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        \n",
    "        found = False\n",
    "        for ext in SUPPORTED_FORMATS:\n",
    "            if filename.lower().endswith(ext):\n",
    "                audio_path = os.path.join(audio_dir, filename)\n",
    "            else:\n",
    "                audio_path = os.path.join(audio_dir, f\"{filename}{ext}\")\n",
    "            \n",
    "            if os.path.exists(audio_path):\n",
    "                valid_files.append({\n",
    "                    'filename': filename,\n",
    "                    'audio_path': audio_path,\n",
    "                    'transcription': row['transcription']\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            invalid_files.append(filename)\n",
    "    \n",
    "    if invalid_files:\n",
    "        print(f\"Advertencia: {len(invalid_files)} archivos no encontrados:\")\n",
    "        for f in invalid_files[:5]:\n",
    "            print(f\"   - {f}\")\n",
    "        if len(invalid_files) > 5:\n",
    "            print(f\"   ... y {len(invalid_files) - 5} m√°s\")\n",
    "    \n",
    "    print(f\"Archivos v√°lidos: {len(valid_files)}/{len(metadata_df)}\")\n",
    "    return valid_files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset(sample_fraction=0.3, random_seed=42):\n",
    "    \"\"\"\n",
    "    Carga el CSV, valida los archivos de audio, toma una fracci√≥n aleatoria\n",
    "    y prepara el dataset dividido en train / validation / test.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(METADATA_FILE, encoding='latin-1')\n",
    "    print(f\"Total de registros en CSV: {len(df)}\")\n",
    "    print(f\"Buscando archivos en: {AUDIO_DIR}\")\n",
    "    valid_files = validate_audio_files(AUDIO_DIR, df)\n",
    "    if not valid_files:\n",
    "        raise ValueError(\"No se encontraron archivos de audio v√°lidos!\")\n",
    "    \n",
    "    print(f\"Total de archivos v√°lidos encontrados: {len(valid_files)}\")\n",
    "\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    sample_size = int(len(valid_files) * sample_fraction)\n",
    "    sampled_files = random.sample(valid_files, sample_size)\n",
    "    print(f\"Usando una muestra aleatoria del {sample_fraction*100:.0f}% \"\n",
    "          f\"({sample_size} archivos)\")\n",
    "\n",
    "    data = {\n",
    "        'audio': [item['audio_path'] for item in sampled_files],\n",
    "        'transcription': [item['transcription'] for item in sampled_files]\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    \n",
    "\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLE_RATE))\n",
    "\n",
    "    train_test = dataset.train_test_split(test_size=0.3, seed=random_seed)\n",
    "    test_valid = train_test['test'].train_test_split(test_size=0.5, seed=random_seed)\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "        'train': train_test['train'],\n",
    "        'validation': test_valid['train'],\n",
    "        'test': test_valid['test']\n",
    "    })\n",
    "\n",
    "    print(f\" Divisi√≥n del dataset:\")\n",
    "    print(f\"Entrenamiento: {len(dataset['train'])} muestras\")\n",
    "    print(f\"Validaci√≥n:    {len(dataset['validation'])} muestras\")\n",
    "    print(f\"Prueba:        {len(dataset['test'])} muestras\")\n",
    "    \n",
    "    return dataset\n",
    "dataset = prepare_dataset(sample_fraction=0.10)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_for_training(batch):\n",
    "    \"\"\"\n",
    "    Preprocesa los audios y transcripciones en BATCHES\n",
    "    Maneja autom√°ticamente diferentes formatos\n",
    "    \"\"\"\n",
    "    audio_arrays = [audio[\"array\"] for audio in batch[\"audio\"]]\n",
    "    sampling_rates = [audio[\"sampling_rate\"] for audio in batch[\"audio\"]]\n",
    "    \n",
    "    input_features = []\n",
    "    for audio_array, sr in zip(audio_arrays, sampling_rates):\n",
    "        features = feature_extractor(\n",
    "            audio_array, \n",
    "            sampling_rate=sr\n",
    "        ).input_features[0]\n",
    "        input_features.append(features)\n",
    "    \n",
    "    batch[\"input_features\"] = input_features\n",
    "    \n",
    "    batch[\"labels\"] = [tokenizer(transcription).input_ids for transcription in batch[\"transcription\"]]\n",
    "    \n",
    "    return batch\n",
    "print(\"Preprocesando dataset por batches\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    pred_ids = np.asarray(pred_ids)\n",
    "    label_ids = np.asarray(label_ids)\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe un archivo de audio\"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
    "\n",
    "    input_features = processor(\n",
    "        waveform.squeeze().numpy(),\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    input_features = input_features.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(\n",
    "        predicted_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path, processor=None, model=None, device=None):\n",
    "    \"\"\"\n",
    "    Transcribe audio usando el modelo Whisper fine-tuned\n",
    "    \"\"\"\n",
    "    if processor is None or model is None:\n",
    "        processor, model, device = load_whisper_model()\n",
    "    \n",
    "    print(f\"üéß Procesando audio: {audio_path}\")\n",
    "    \n",
    "\n",
    "    audio, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    input_features = processor(\n",
    "        audio, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    transcription = processor.batch_decode(\n",
    "        predicted_ids, \n",
    "        skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "def load_whisper_model(model_path=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Carga el modelo Whisper fine-tuned y el procesador\n",
    "    \"\"\"\n",
    "    print(f\"üì• Cargando modelo Whisper desde {model_path}...\")\n",
    "    processor = WhisperProcessor.from_pretrained(model_path)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    print(f\"‚úÖ Modelo cargado en {device}\")\n",
    "    \n",
    "    return processor, model, device\n",
    "\n",
    "def analyze_sentiment_multilang(text):\n",
    "    lang = detect_language(text)\n",
    "    emotion_classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"SamLowe/roberta-base-go_emotions\",\n",
    "        top_k=5  \n",
    "    )\n",
    "\n",
    "    if lang == 'es':\n",
    "        sentiment_model = \"Hate-speech-CNERG/dehatebert-mono-spanish\"\n",
    "    elif lang == 'en':\n",
    "        sentiment_model = \"cardiffnlp/twitter-roberta-base-offensive\"\n",
    "    else:\n",
    "        sentiment_model = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    \n",
    "    sentiment_classifier = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=sentiment_model\n",
    "    )\n",
    "    \n",
    "    emotions = emotion_classifier(text)[0]\n",
    "    sentiment = sentiment_classifier(text)[0]\n",
    "    \n",
    "    return {\n",
    "        \"language\": lang,\n",
    "        \"text\": text,\n",
    "        \"emotions\": emotions,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"alert_level\": calculate_alert_level(emotions)\n",
    "    }\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detecta autom√°ticamente el idioma del texto\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return 'en' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0f03a",
   "metadata": {},
   "source": [
    "### **Carga Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc63f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 16000\n",
      "Labels: ('-', '|', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z')\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.generation_config.language = LANGUAGE\n",
    "model.generation_config.task = TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f75a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /home/santenana/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 360M/360M [00:03<00:00, 103MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "try:\n",
    "    dataset = dataset.map(\n",
    "        prepare_dataset_for_training,\n",
    "        remove_columns=dataset.column_names[\"train\"],\n",
    "        batched=True,  \n",
    "        batch_size=BATCH_SIZE,  \n",
    "        num_proc=1, \n",
    "        desc=\"Procesando audios\"  \n",
    "    )\n",
    "    print(\"‚úÖ Preprocesamiento completado\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error durante preprocesamiento: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c19713",
   "metadata": {},
   "source": [
    "### **Entrenar modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(output_dir=OUTPUT_DIR,\n",
    "                                         per_device_train_batch_size=8, \n",
    "                                         gradient_accumulation_steps=2,\n",
    "                                         learning_rate=1e-5,\n",
    "                                         warmup_steps=50,\n",
    "                                         max_steps=500,  \n",
    "                                         gradient_checkpointing=False, #True,\n",
    "                                         fp16=True,#torch.cuda.is_available(),\n",
    "                                         eval_strategy=\"steps\",\n",
    "                                         per_device_eval_batch_size=8,\n",
    "                                         predict_with_generate=True,\n",
    "                                         generation_max_length=225,\n",
    "                                         save_steps=100,\n",
    "                                         eval_steps=100,\n",
    "                                         logging_steps=30,\n",
    "                                         report_to=[\"tensorboard\"],\n",
    "                                         load_best_model_at_end=True,\n",
    "                                         metric_for_best_model=\"wer\",\n",
    "                                         greater_is_better=False,\n",
    "                                         push_to_hub=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f22a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor)\n",
    "#tokenizer=processor.feature_extractor)\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Modelo guardado en {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './whisper-finetuned-transcipt-es'\n",
    "processor, model, device = load_whisper_model(model_path)\n",
    "\n",
    "audio_path = '/home/santenana/Proyectos_ML_DC/03_Audio_to_Speech/test_5.wav'\n",
    "\n",
    "text = transcribe_audio(audio_path, processor, model, device)\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
